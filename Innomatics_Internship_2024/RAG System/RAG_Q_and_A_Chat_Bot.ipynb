{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eefe6a54-b3f1-480a-aa64-3297c14c7994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ebaed362-3dc5-41ec-874d-a6f862882f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Setup API Key\n",
    "f = open('data/.api_key_dp.txt')\n",
    "API_KEY = f.read()\n",
    "\n",
    "chat_model = ChatGoogleGenerativeAI(google_api_key = API_KEY, model = 'gemini-1.5-pro-latest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da2fe28-95d9-4451-81c0-3f204bffeaba",
   "metadata": {},
   "source": [
    "## Load Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cde03451-a2ab-427a-b5d1-399968256024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a document\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"data/2404.07143.pdf\")\n",
    "\n",
    "data = loader.load_and_split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76851de5-561e-48e6-970a-3de050dd4cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Preprint. Under review.\\nLeave No Context Behind:\\nEfficient Infinite Context Transformers with Infini-attention\\nTsendsuren Munkhdalai, Manaal Faruqui and Siddharth Gopal\\nGoogle\\ntsendsuren@google.com\\nAbstract\\nThis work introduces an efficient method to scale Transformer-based Large\\nLanguage Models (LLMs) to infinitely long inputs with bounded memory\\nand computation. A key component in our proposed approach is a new at-\\ntention technique dubbed Infini-attention. The Infini-attention incorporates\\na compressive memory into the vanilla attention mechanism and builds\\nin both masked local attention and long-term linear attention mechanisms\\nin a single Transformer block. We demonstrate the effectiveness of our\\napproach on long-context language modeling benchmarks, 1M sequence\\nlength passkey context block retrieval and 500K length book summarization\\ntasks with 1B and 8B LLMs. Our approach introduces minimal bounded\\nmemory parameters and enables fast streaming inference for LLMs.\\n1 Introduction\\nMemory serves as a cornerstone of intelligence, as it enables efficient computations tailored\\nto specific contexts. However, Transformers (Vaswani et al., 2017) and Transformer-based\\nLLMs (Brown et al., 2020; Touvron et al., 2023; Anil et al., 2023; Groeneveld et al., 2024) have\\na constrained context-dependent memory, due to the nature of the attention mechanism.\\nUpdate \\nVVConcat Concat \\nQ V\\nQ V\\nQs{KV}sCompressive memory & \\nLinear attention Causal scaled dot-product \\nattention & PE Linear \\nprojection \\n{KV}s-1Retrieve \\nFigure 1: Infini-attention has an addi-\\ntional compressive memory with linear\\nattention for processing infinitely long\\ncontexts. {KV}sâˆ’1and{KV}sare atten-\\ntion key and values for current and previ-\\nous input segments, respectively and Qs\\nthe attention queries. PE denotes position\\nembeddings.The attention mechanism in Transformers ex-\\nhibits quadratic complexity in both memory\\nfootprint and computation time. For example,\\nthe attention Key-Value (KV) states have 3TB\\nmemory footprint for a 500B model with batch\\nsize 512 and context length 2048 (Pope et al.,\\n2023). Indeed, scaling LLMs to longer sequences\\n(i.e. 1M tokens) is challenging with the standard\\nTransformer architectures and serving longer\\nand longer context models becomes costly finan-\\ncially.\\nCompressive memory systems promise to be\\nmore scalable and efficient than the attention\\nmechanism for extremely long sequences (Kan-\\nerva, 1988; Munkhdalai et al., 2019). Instead\\nof using an array that grows with the input se-\\nquence length, a compressive memory primarily\\nmaintains a fixed number of parameters to store\\nand recall information with a bounded storage\\nand computation costs. In the compressive mem-\\nory, new information is added to the memory\\nby changing its parameters with an objective\\nthat this information can be recovered back later\\non. However, the LLMs in their current state\\nhave yet to see an effective, practical compres-\\nsive memory technique that balances simplicity along with quality.\\n1arXiv:2404.07143v1  [cs.CL]  10 Apr 2024', metadata={'source': 'data/2404.07143.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a48e90ab-b9fd-40bb-b45a-8c2ecf45e697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This step is to Avoid Runtime Error in the next step\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d05c1a9-7558-4c5b-80eb-93a7aa216f7b",
   "metadata": {},
   "source": [
    "## Spliting the document into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "605b457a-2551-431d-aeb7-8da4b788a9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 568, which is longer than the specified 500\n",
      "Created a chunk of size 506, which is longer than the specified 500\n",
      "Created a chunk of size 633, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import NLTKTextSplitter\n",
    "\n",
    "split = NLTKTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "chunks = split.split_documents(data)\n",
    "\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc12f73-7137-4df3-9e41-43ca67dd6f55",
   "metadata": {},
   "source": [
    "## Chunks into Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b472c4d-a0d3-4bb9-b461-be9a2c3c9138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(google_api_key=API_KEY, model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f870c50a-131a-4e43-8fb0-3376e2bbd0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f5a3914-2fff-4639-80ab-8bf4a4618f17",
   "metadata": {},
   "source": [
    "## Storing the chunks in vector form in Croma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91b1734e-ebb3-45a8-af6c-e8a3d7bada47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "db = Chroma.from_documents(chunks, embedding_model, persist_directory=\"data/chroma_db_for_rag\")\n",
    "\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "824980c1-fe95-4cdb-99a9-88f307a197e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = Chroma(persist_directory=\"data/chroma_db_for_rag\", embedding_function=embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08e87b0-cbdc-4aa0-bb95-86862ae59b6c",
   "metadata": {},
   "source": [
    "## Settingup the Vector Store as a Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "54142b04-a527-4df2-85af-c7319bf7f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = connection.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd77a47d-2461-4aa9-810a-57020ef5da9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x00000179CA1681C0>, search_kwargs={'k': 5})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d17bee3-abfc-4f7c-8dfd-d7c764812d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8750cbe4-215b-400a-8dfc-58c0416a94d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What is Large Language Model?\"\n",
    "\n",
    "retrieve_docs = retriever.invoke(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34c0ee13-115e-4e54-9c3d-9230a4296e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieve_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5cfc8cfd-80d3-43ed-a613-ab240721744d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='However, the LLMs in their current state\\nhave yet to see an effective, practical compres-\\nsive memory technique that balances simplicity along with quality.\\n\\n1arXiv:2404.07143v1  [cs.CL]  10 Apr 2024', metadata={'page': 0, 'source': 'data/2404.07143.pdf'}),\n",
       " Document(page_content='However, the LLMs in their current state\\nhave yet to see an effective, practical compres-\\nsive memory technique that balances simplicity along with quality.\\n\\n1arXiv:2404.07143v1  [cs.CL]  10 Apr 2024', metadata={'page': 0, 'source': 'data/2404.07143.pdf'}),\n",
       " Document(page_content='However, the LLMs in their current state\\nhave yet to see an effective, practical compres-\\nsive memory technique that balances simplicity along with quality.\\n\\n1arXiv:2404.07143v1  [cs.CL]  10 Apr 2024', metadata={'page': 0, 'source': 'data/2404.07143.pdf'}),\n",
       " Document(page_content='Effective\\nlong-context scaling of foundation models.\\n\\narXiv preprint arXiv:2309.16039 , 2023.\\n\\nA Additional Training Details\\nFor the long-context language modeling task, we set the learning rate to 0.01 by perform-\\ning small search over values of 0.003, 0.005, 0.01 and 0.03.\\n\\nWe used the Adafactor opti-\\nmizer (Shazeer & Stern, 2018) with linear warmup with 1000 steps, followed by cosine\\ndecay.\\n\\nWe applied gradient checkpointing after each segment to save to save memory.', metadata={'page': 11, 'source': 'data/2404.07143.pdf'}),\n",
       " Document(page_content='Advances\\nin Neural Information Processing Systems , 35:11079â€“11091, 2022.\\n\\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian.\\n\\nExtending con-\\ntext window of large language models via positional interpolation.\\n\\narXiv preprint\\narXiv:2306.15595 , 2023a.\\n\\nYukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia.\\n\\nLonglora: Efficient fine-tuning of long-context large language models.\\n\\narXiv preprint\\narXiv:2309.12307 , 2023b.', metadata={'page': 8, 'source': 'data/2404.07143.pdf'})]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e9961ae-e9c0-4d3b-840d-6fe1ab6c1dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'However, the LLMs in their current state\\nhave yet to see an effective, practical compres-\\nsive memory technique that balances simplicity along with quality.\\n\\n1arXiv:2404.07143v1  [cs.CL]  10 Apr 2024'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa44189-6c41-4efb-a7f9-ccd543d6f78f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bda5262-179d-4972-bbfb-8896fc1f9914",
   "metadata": {},
   "source": [
    "## context and questioning to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "999d104a-d89c-46a1-ba72-86285d926da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    # System Message Prompt Template\n",
    "    SystemMessage(content=\"\"\"You are a Helpful AI Bot. \n",
    "    You take the context and question from user. Your answer should be based on the specific context.\"\"\"),\n",
    "    # Human Message Prompt Template\n",
    "    HumanMessagePromptTemplate.from_template(\"\"\"Answer the question based on the given context.\n",
    "    Context: {context} Question: {question} Answer: \"\"\") ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2648a1c-e3c5-4234-b777-0a5d93f282db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "883825c8-27fc-432d-ac9c-db4b3e35a0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | chat_template\n",
    "    | chat_model\n",
    "    | output_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f28ba637-37dc-4509-a45e-39c1e6c5ed6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Transformers: A Summary from the Context\n",
       "\n",
       "Based on the provided text, **Transformers** appear to be a type of neural network architecture with a focus on efficient and extensive context utilization. Here's what we can gather:\n",
       "\n",
       "* **Relationship to RNNs:** The paper \"Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention\" suggests a connection between Transformers and Recurrent Neural Networks (RNNs), potentially in terms of sequence processing capabilities.\n",
       "* **Context and Memory:**  The context seems crucial for Transformers. They employ mechanisms like:\n",
       "    * **Storing KV states:**  Memorizing Transformers store Key-Value pairs for context, but this becomes costly, limiting it to a single layer.\n",
       "    * **kNN retrieval:**  A fast k-Nearest Neighbors retrieval method helps build a broader context window, covering the entire sequence history at the cost of increased storage.\n",
       "    * **Context window extension:** Techniques like Transformer-XL and Compressive Transformer extend the context window beyond a single layer, allowing access to a more extensive history of tokens with varying memory footprints.\n",
       "* **Efficiency Considerations:**  The text highlights the trade-off between context size and storage/memory requirements. Different approaches balance these factors to optimize performance. \n",
       "\n",
       "**Overall, Transformers seem to be powerful tools for sequence processing tasks where context plays a significant role.  They utilize various methods to efficiently incorporate past information, making them suitable for complex tasks requiring long-range dependencies.** \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as markdown\n",
    "response = rag_chain.invoke(\"What is Transformers?\")\n",
    "\n",
    "markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6a65e590-c6ba-4f2a-9fa3-27173ebdc73b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Understanding Large Language Models (LLMs) based on the Context\n",
       "\n",
       "The provided text snippets offer valuable insights into Large Language Models (LLMs) and their current limitations. While a direct definition isn't provided, we can infer the following:\n",
       "\n",
       "**LLMs are complex AI models capable of processing and generating human-like text.** They are trained on massive datasets and can perform various tasks such as translation, summarization, and question answering.\n",
       "\n",
       "**Current Challenges for LLMs:**\n",
       "\n",
       "* **Limited Contextual Memory:** The context highlights a critical challenge: LLMs struggle with effective and practical memory techniques. This means they have difficulty retaining and utilizing information from long passages or conversations, impacting their ability to understand complex contexts and generate coherent responses. \n",
       "* **Balancing Simplicity and Quality:**  Researchers are actively seeking methods to improve LLM memory while maintaining simplicity and quality. This involves exploring techniques like \"compressive memory\" to efficiently store and access information without sacrificing performance.\n",
       "\n",
       "**Research Efforts and Directions:**\n",
       "\n",
       "The references to various research papers suggest ongoing efforts to address these challenges:\n",
       "\n",
       "* **Long-context scaling:** Exploring methods to effectively scale LLMs for handling longer contexts.\n",
       "* **Positional interpolation:** Investigating techniques to extend the context window of LLMs. \n",
       "* **Efficient fine-tuning:** Developing methods like \"LongLoRA\" for efficient fine-tuning of LLMs with long contexts.\n",
       "\n",
       "**In conclusion,** LLMs hold immense potential but require further development, particularly in enhancing their contextual memory capabilities. Researchers are actively exploring solutions to improve their efficiency and effectiveness in handling complex tasks and longer sequences of information. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"What is Large Language Model?\")\n",
    "\n",
    "markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a71a18-c539-4937-807c-6eacea13aedc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
